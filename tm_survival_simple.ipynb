{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hf8585/.conda/envs/torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import h5py as h5\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append(os.path.expanduser(\"~/TMPredictor/survival_tm/auton-survival\"))\n",
    "from auton_survival.preprocessing import Scaler\n",
    "import optuna\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "#sys.path.append('/projects/EKOLEMEN/survival_tm/train_models/auton-survival')\n",
    "sys.path.append(os.path.expanduser(\"~/TMPredictor/survival_tm/auton-survival\"))\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from auton_survival.estimators import SurvivalModel\n",
    "from auton_survival.metrics import survival_regression_metric\n",
    "from auton_survival.models.dsm import DeepSurvivalMachines\n",
    "from sksurv.metrics import concordance_index_ipcw, brier_score, cumulative_dynamic_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shots = np.load('/projects/EKOLEMEN/survival_tm/shots.npy')\n",
    "tm_shots = np.load('/projects/EKOLEMEN/survival_tm/tm_shots.npy')\n",
    "st_shots = np.load('/projects/EKOLEMEN/survival_tm/st_shots.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_type):\n",
    "    with open(f'/projects/EKOLEMEN/survival_tm/formatted_labels/{data_type}.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    return data['x'], data['t'], data['e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't want whole dataset right now so 0.5 factor to reduce\n",
    "n = len(shots)\n",
    "\n",
    "tr_size = int(n*0.80)\n",
    "vl_size = int(n*0.10)\n",
    "te_size = int(n*0.10)\n",
    "\n",
    "train_shots = shots[:tr_size]\n",
    "test_shots = shots[-te_size:]\n",
    "valid_shots = shots[tr_size:tr_size+vl_size]\n",
    "\n",
    "x_train, t_train, e_train = load_data('train')\n",
    "x_test,  t_test,  e_test  = load_data('test')\n",
    "x_valid, t_valid, e_valid = load_data('valid')\n",
    "\n",
    "# Get inds for time <600ms\n",
    "# is this needed? Not sure.\n",
    "'''inds = np.where(t_train < 600)[0]\n",
    "\n",
    "x_train = x_train[inds]\n",
    "t_train = t_train[inds]\n",
    "e_train = e_train[inds]'''\n",
    "\n",
    "tm_inds = np.where(e_train == 1)[0]\n",
    "st_inds = np.where(e_train == 0)[0]\n",
    "new_st_inds = np.random.choice(st_inds, size=len(tm_inds), replace=False)\n",
    "\n",
    "x_train = np.concatenate((x_train[tm_inds], x_train[new_st_inds]), axis=0)\n",
    "t_train = np.concatenate((t_train[tm_inds], t_train[new_st_inds]), axis=0)\n",
    "e_train = np.concatenate((e_train[tm_inds], e_train[new_st_inds]), axis=0)\n",
    "\n",
    "# Shuffle arrays because currently all 1s followed by all 0s\n",
    "p = np.random.permutation(len(t_train))\n",
    "x_train = x_train[p,:]\n",
    "t_train = t_train[p]\n",
    "e_train = e_train[p]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we predict 200ms in the future, the model should see the actuator trajectories 200ms in the future. Don't show betan, just actuators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df = pd.DataFrame(x_train)\n",
    "t_train_df = pd.DataFrame(t_train)\n",
    "e_train_df = pd.DataFrame(e_train)\n",
    "\n",
    "x_valid_df = pd.DataFrame(x_valid)\n",
    "t_valid_df = pd.DataFrame(t_valid)\n",
    "e_valid_df = pd.DataFrame(e_valid)\n",
    "\n",
    "x_test_df = pd.DataFrame(x_test)\n",
    "t_test_df = pd.DataFrame(t_test)\n",
    "e_test_df = pd.DataFrame(e_test)\n",
    "\n",
    "outcomes_valid_df = pd.DataFrame({'time': t_valid, 'event': e_valid})\n",
    "\n",
    "outcomes_df = pd.DataFrame({'time': t_train, 'event': e_train})\n",
    "\n",
    "#normalize\n",
    "scaler = Scaler()\n",
    "transformer = scaler.fit(x_train_df)\n",
    "x_train_df = transformer.transform(x_train_df)\n",
    "x_valid_df = transformer.transform(x_valid_df)\n",
    "x_test_df = transformer.transform(x_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1819/10000 [06:17<28:19,  4.81it/s]  \n",
      "100%|██████████| 200/200 [3:21:10<00:00, 60.35s/it]  \n",
      "/home/hf8585/TMPredictor/survival_tm/auton-survival/auton_survival/models/dsm/losses.py:406: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  t_horz = torch.tensor(t_horizon).double().to(logits.device)\n",
      "/home/hf8585/TMPredictor/survival_tm/auton-survival/auton_survival/estimators.py:206: FutureWarning: DataFrame.interpolate with method=bfill is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  return survival_predictions.sort_index(axis=0).interpolate().interpolate(method='bfill').T[times].values\n",
      "/home/hf8585/TMPredictor/survival_tm/auton-survival/auton_survival/metrics.py:212: UserWarning: You are are evaluating model performance on the same data used to estimate the censoring distribution.\n",
      "  warnings.warn(\"You are are evaluating model performance on the \\\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input estimate contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/hf8585/TMPredictor/survival_tm/TearingModeSurvival/tm_survival_simple.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstellar.princeton.edu/home/hf8585/TMPredictor/survival_tm/TearingModeSurvival/tm_survival_simple.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Obtain survival probabilities for validation set and compute the Integrated Brier Score \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstellar.princeton.edu/home/hf8585/TMPredictor/survival_tm/TearingModeSurvival/tm_survival_simple.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m predictions_val \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict_survival(x_valid_df, times)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bstellar.princeton.edu/home/hf8585/TMPredictor/survival_tm/TearingModeSurvival/tm_survival_simple.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m metric_val \u001b[39m=\u001b[39m survival_regression_metric(\u001b[39m'\u001b[39m\u001b[39mibs\u001b[39m\u001b[39m'\u001b[39m, outcomes_valid_df, predictions_val, times)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstellar.princeton.edu/home/hf8585/TMPredictor/survival_tm/TearingModeSurvival/tm_survival_simple.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m models\u001b[39m.\u001b[39mappend([metric_val, train_loss, val_loss, model])\n",
      "File \u001b[0;32m~/TMPredictor/survival_tm/auton-survival/auton_survival/metrics.py:235\u001b[0m, in \u001b[0;36msurvival_regression_metric\u001b[0;34m(metric, outcomes, predictions, times, outcomes_train, n_bootstrap, random_seed)\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m()\n\u001b[1;32m    234\u001b[0m \u001b[39mif\u001b[39;00m n_bootstrap \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 235\u001b[0m   \u001b[39mreturn\u001b[39;00m _metric(survival_train, survival_test, predictions, times)\n\u001b[1;32m    236\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m   \u001b[39mreturn\u001b[39;00m [_metric(survival_train, survival_test, predictions, times, random_seed\u001b[39m=\u001b[39mi) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_bootstrap)]\n",
      "File \u001b[0;32m~/TMPredictor/survival_tm/auton-survival/auton_survival/metrics.py:256\u001b[0m, in \u001b[0;36m_integrated_brier_score\u001b[0;34m(survival_train, survival_test, predictions, times, random_seed)\u001b[0m\n\u001b[1;32m    253\u001b[0m   np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(random_seed)\n\u001b[1;32m    254\u001b[0m   idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(idx, \u001b[39mlen\u001b[39m(predictions), replace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 256\u001b[0m \u001b[39mreturn\u001b[39;00m metrics\u001b[39m.\u001b[39mintegrated_brier_score(survival_train, survival_test[idx], predictions[idx], times)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.11/site-packages/sksurv/metrics.py:735\u001b[0m, in \u001b[0;36mintegrated_brier_score\u001b[0;34m(survival_train, survival_test, estimate, times)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"The Integrated Brier Score (IBS) provides an overall calculation of\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[39mthe model performance at all available times :math:`t_1 \\\\leq t \\\\leq t_\\\\text{max}`.\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[39m       Statistics in Medicine, vol. 18, no. 17-18, pp. 2529–2545, 1999.\u001b[39;00m\n\u001b[1;32m    733\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    734\u001b[0m \u001b[39m# Computing the brier scores\u001b[39;00m\n\u001b[0;32m--> 735\u001b[0m times, brier_scores \u001b[39m=\u001b[39m brier_score(survival_train, survival_test, estimate, times)\n\u001b[1;32m    737\u001b[0m \u001b[39mif\u001b[39;00m times\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    738\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAt least two time points must be given\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.11/site-packages/sksurv/metrics.py:609\u001b[0m, in \u001b[0;36mbrier_score\u001b[0;34m(survival_train, survival_test, estimate, times)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Estimate the time-dependent Brier score for right censored data.\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \n\u001b[1;32m    516\u001b[0m \u001b[39mThe time-dependent Brier score is the mean squared error at time point :math:`t`:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[39m       Statistics in Medicine, vol. 18, no. 17-18, pp. 2529–2545, 1999.\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    608\u001b[0m test_event, test_time \u001b[39m=\u001b[39m check_y_survival(survival_test)\n\u001b[0;32m--> 609\u001b[0m estimate, times \u001b[39m=\u001b[39m _check_estimate_2d(estimate, test_time, times, estimator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbrier_score\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    610\u001b[0m \u001b[39mif\u001b[39;00m estimate\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m times\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    611\u001b[0m     estimate \u001b[39m=\u001b[39m estimate\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.11/site-packages/sksurv/metrics.py:76\u001b[0m, in \u001b[0;36m_check_estimate_2d\u001b[0;34m(estimate, test_time, time_points, estimator)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_estimate_2d\u001b[39m(estimate, test_time, time_points, estimator):\n\u001b[0;32m---> 76\u001b[0m     estimate \u001b[39m=\u001b[39m check_array(estimate, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, allow_nd\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mestimate\u001b[39m\u001b[39m\"\u001b[39m, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m     77\u001b[0m     time_points \u001b[39m=\u001b[39m _check_times(test_time, time_points)\n\u001b[1;32m     78\u001b[0m     check_consistent_length(test_time, estimate)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.11/site-packages/sklearn/utils/validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    952\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    953\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    954\u001b[0m         )\n\u001b[1;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 957\u001b[0m         _assert_all_finite(\n\u001b[1;32m    958\u001b[0m             array,\n\u001b[1;32m    959\u001b[0m             input_name\u001b[39m=\u001b[39minput_name,\n\u001b[1;32m    960\u001b[0m             estimator_name\u001b[39m=\u001b[39mestimator_name,\n\u001b[1;32m    961\u001b[0m             allow_nan\u001b[39m=\u001b[39mforce_all_finite \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    962\u001b[0m         )\n\u001b[1;32m    964\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    965\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.11/site-packages/sklearn/utils/validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    123\u001b[0m     X,\n\u001b[1;32m    124\u001b[0m     xp\u001b[39m=\u001b[39mxp,\n\u001b[1;32m    125\u001b[0m     allow_nan\u001b[39m=\u001b[39mallow_nan,\n\u001b[1;32m    126\u001b[0m     msg_dtype\u001b[39m=\u001b[39mmsg_dtype,\n\u001b[1;32m    127\u001b[0m     estimator_name\u001b[39m=\u001b[39mestimator_name,\n\u001b[1;32m    128\u001b[0m     input_name\u001b[39m=\u001b[39minput_name,\n\u001b[1;32m    129\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.11/site-packages/sklearn/utils/validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    155\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    158\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input estimate contains NaN."
     ]
    }
   ],
   "source": [
    "param_grid = {'k' : [3],\n",
    "              'iters': [200],\n",
    "              'distribution' : ['LogNormal'],\n",
    "              'learning_rate' : [ 1e-3 ],\n",
    "              'batch_size' : [10000],\n",
    "              'layers' : [[100, 60, 175, 225, 120]]\n",
    "             }\n",
    "\n",
    "params = ParameterGrid(param_grid)\n",
    "models=[]\n",
    "for param in params:\n",
    "    model = SurvivalModel(model='dsm', \n",
    "                      iters=param['iters'], \n",
    "                      k=param['k'], \n",
    "                      layers=param['layers'], \n",
    "                      distribution=param['distribution'],\n",
    "                      learning_rate=param['learning_rate'], \n",
    "                      batch_size=param['batch_size']\n",
    "                    )\n",
    "    '''model = SurvivalModel(model='dsm', \n",
    "                          iters=param['iters'], \n",
    "                          k=param['k'], \n",
    "                          layers=param['layers'], \n",
    "                          distribution=param['distribution'],\n",
    "                          learning_rate=param['learning_rate'], \n",
    "                          batch_size=param['batch_size']\n",
    "                        )'''\n",
    "    _, train_loss, val_loss = model.fit(x_train_df, outcomes_df, val_data=(x_valid_df, outcomes_valid_df))\n",
    "\n",
    "    # Obtain survival probabilities for validation set and compute the Integrated Brier Score \n",
    "    predictions_val = model.predict_survival(x_valid_df, times)\n",
    "    metric_val = survival_regression_metric('ibs', outcomes_valid_df, predictions_val, times)\n",
    "    models.append([metric_val, train_loss, val_loss, model])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<auton_survival.estimators.SurvivalModel object at 0x145b38bbcbd0>\n"
     ]
    }
   ],
   "source": [
    "with open('models.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "print(loaded_model[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('survival.pkl', 'rb') as f:\n",
    "    out_survival = pickle.load(f)\n",
    "\n",
    "#out_risk = model.predict_risk(pd.DataFrame(x_test), times)\n",
    "#out_survival = model.predict_survival(pd.DataFrame(x_test), times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_peaks_in_data(data):\n",
    "    peaks = []\n",
    "    for i in range(1, len(data) - 1):\n",
    "        if data[i-1] < data[i] > data[i+1]:\n",
    "            peaks.append(i)\n",
    "    return peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_times = [20, 50, 100, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outcomes_train=pd.DataFrame({'time': t_train, 'event': e_train})\n",
    "for i in range(len(loaded_model)):\n",
    "    out_survival = loaded_model[i][0].predict_survival(x_valid_df, prediction_times)\n",
    "    score = survival_regression_metric(metric='auc', outcomes=outcomes_valid_df, predictions=out_survival, times=prediction_times)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_survival = models[0][3].predict_survival(x_test_df, prediction_times)\n",
    "peaks = find_peaks_in_data(t_test)\n",
    "for i in range(0, 3):\n",
    "    peak_number = 140 + i\n",
    "    start_index = peaks[peak_number]\n",
    "    end_index = peaks[peak_number + 1]\n",
    "    times = np.arange(0, (end_index - start_index)*20, 20)\n",
    "    plt.plot(times, out_survival[start_index:end_index,2], label='Survival in 100ms')\n",
    "    plt.plot(times, out_survival[start_index:end_index,3], label='Survival in 200ms')\n",
    "\n",
    "    if (e_test[start_index]==1):\n",
    "        plt.title('YES TM')\n",
    "    else:\n",
    "        plt.title('NO TM')\n",
    "    plt.xlabel('Time / ms')\n",
    "    plt.ylabel('Survival probability')\n",
    "    plt.ylim(0.5, 1)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make WTC curve\n",
    "\n",
    "# one issue with this metric is if we never predict TMs then we're always perfect. \n",
    "def fpr_auc(model, normed_x, normed_t, normed_e, prediction_times, threshold=0.7):\n",
    "    out_survival = model.predict_survival(normed_x, prediction_times)\n",
    "    fprs = []\n",
    "    for i, time in enumerate(prediction_times):\n",
    "        survival_prediction = out_survival[:,i]\n",
    "        survival_prediction = (survival_prediction < threshold).astype(int)\n",
    "        false_positives = np.logical_and(survival_prediction == 1, normed_e == 0)\n",
    "        true_negatives = np.logical_and(survival_prediction == 0, normed_e == 0)\n",
    "        fpr = false_positives.sum() / (false_positives.sum() + true_negatives.sum())\n",
    "        fprs.append(fpr)\n",
    "\n",
    "    auc = np.trapz(fprs, prediction_times)\n",
    "    return auc, fprs, prediction_times\n",
    "\n",
    "def find_peaks_in_data(data):\n",
    "    peaks = []\n",
    "    for i in range(1, len(data) - 1):\n",
    "        if data[i-1] < data[i] > data[i+1]:\n",
    "            peaks.append(i)\n",
    "    return peaks\n",
    "\n",
    "def fnr_auc(model, normed_x, normed_t, normed_e, prediction_times, threshold=0.7):\n",
    "    out_survival = model.predict_survival(normed_x, prediction_times)\n",
    "    fnrs = []\n",
    "    shot_indices = find_peaks_in_data(normed_t)\n",
    "    for i, time in enumerate(prediction_times):\n",
    "        tm_prediction_per_shot = []\n",
    "        # 1 means correct TM prediction, 0 means unpredicted TM, -1 means no TM in shot\n",
    "        # a TM is predicted when the survival prediction is 0 at any point in the shot. Check if better results when TM is consecutive 0s\n",
    "        for j, shot_index in enumerate(shot_indices):\n",
    "            survival_prediction = out_survival[:,i]\n",
    "            survival_prediction = (survival_prediction < threshold).astype(int)\n",
    "            tm = (0 in survival_prediction[shot_indices[j]:shot_indices[j+1]])\n",
    "            if normed_e[shot_index] == 1 and tm:\n",
    "                tm_prediction_per_shot.append(1)\n",
    "            elif normed_e[shot_index] == 1 and not tm:\n",
    "                tm_prediction_per_shot.append(0)\n",
    "            else:\n",
    "                tm_prediction_per_shot.append(-1)\n",
    "        fnr = tm_prediction_per_shot.count(0) / (tm_prediction_per_shot.count(1) + tm_prediction_per_shot.count(0))\n",
    "        fnrs.append(fnr)\n",
    "    auc = np.trapz(fnrs, prediction_times)\n",
    "    return auc, fnrs, prediction_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hf8585/TMPredictor/survival_tm/auton-survival/auton_survival/estimators.py:206: FutureWarning: DataFrame.interpolate with method=bfill is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  return survival_predictions.sort_index(axis=0).interpolate().interpolate(method='bfill').T[times].values\n",
      "/home/hf8585/TMPredictor/survival_tm/auton-survival/auton_survival/estimators.py:206: FutureWarning: DataFrame.interpolate with method=bfill is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  return survival_predictions.sort_index(axis=0).interpolate().interpolate(method='bfill').T[times].values\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/hf8585/TMPredictor/survival_tm/TearingModeSurvival/tm_survival_simple.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstellar.princeton.edu/home/hf8585/TMPredictor/survival_tm/TearingModeSurvival/tm_survival_simple.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m threshold \u001b[39m=\u001b[39m \u001b[39m0.7\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstellar.princeton.edu/home/hf8585/TMPredictor/survival_tm/TearingModeSurvival/tm_survival_simple.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m fpr_auc, fprs, prediction_times \u001b[39m=\u001b[39m fpr_auc(models[\u001b[39m0\u001b[39m][\u001b[39m3\u001b[39m], x_test_df, t_test, e_test, prediction_times, threshold\u001b[39m=\u001b[39mthreshold)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bstellar.princeton.edu/home/hf8585/TMPredictor/survival_tm/TearingModeSurvival/tm_survival_simple.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m fnr_auc, fnrs, prediction_times \u001b[39m=\u001b[39m fnr_auc(models[\u001b[39m0\u001b[39m][\u001b[39m3\u001b[39m], x_test_df, t_test, e_test, prediction_times, threshold\u001b[39m=\u001b[39mthreshold)\n",
      "\u001b[1;32m/home/hf8585/TMPredictor/survival_tm/TearingModeSurvival/tm_survival_simple.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstellar.princeton.edu/home/hf8585/TMPredictor/survival_tm/TearingModeSurvival/tm_survival_simple.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m survival_prediction \u001b[39m=\u001b[39m out_survival[:,i]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstellar.princeton.edu/home/hf8585/TMPredictor/survival_tm/TearingModeSurvival/tm_survival_simple.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m survival_prediction \u001b[39m=\u001b[39m (survival_prediction \u001b[39m<\u001b[39m threshold)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bstellar.princeton.edu/home/hf8585/TMPredictor/survival_tm/TearingModeSurvival/tm_survival_simple.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m tm \u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m \u001b[39min\u001b[39;00m survival_prediction[shot_indices[j]:shot_indices[j\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstellar.princeton.edu/home/hf8585/TMPredictor/survival_tm/TearingModeSurvival/tm_survival_simple.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mif\u001b[39;00m normed_e[shot_index] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m tm:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstellar.princeton.edu/home/hf8585/TMPredictor/survival_tm/TearingModeSurvival/tm_survival_simple.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     tm_prediction_per_shot\u001b[39m.\u001b[39mappend(\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "threshold = 0.7\n",
    "fpr_auc, fprs, prediction_times = fpr_auc(models[0][3], x_test_df, t_test, e_test, prediction_times, threshold=threshold)\n",
    "fnr_auc, fnrs, prediction_times = fnr_auc(models[0][3], x_test_df, t_test, e_test, prediction_times, threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
